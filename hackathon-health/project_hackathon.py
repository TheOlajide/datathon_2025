# -*- coding: utf-8 -*-
"""Project-Hackathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nLFNMyYBdNt6A00MwRsX_lvDBE-3i1BX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

### for time series
from prophet import Prophet
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

from google.colab import drive
drive.mount('/content/drive')

patient='/content/drive/MyDrive/Project-hackathon-folder/Dataset/patients_dataset.csv'
health_workers='/content/drive/MyDrive/Project-hackathon-folder/Dataset/health_workers_dataset.csv'
inventory='/content/drive/MyDrive/Project-hackathon-folder/Dataset/inventory_dataset.csv'
disease='/content/drive/MyDrive/Project-hackathon-folder/Dataset/disease_report_full.csv'
nigeria='/content/drive/MyDrive/Project-hackathon-folder/Dataset/Nigeria_phc_3200.csv'

def wrangle(filepath):
  df=pd.read_csv(filepath)
  df.drop_duplicates(inplace=True)
  return df

patients_df=wrangle(patient)
workers_df=wrangle(health_workers)
disease_df=wrangle(disease)
inventory_df=wrangle(inventory)
nigeria_df=wrangle(nigeria)
patients_df.head(2)

workers_df.head(2)

disease_df.head(2)

nigeria_df.head(2)

inventory_df.head(2)

patients_df['visit_date']=pd.to_datetime(patients_df['visit_date'])

patients_df['year'] = patients_df['visit_date'].dt.year
patients_df['month'] = patients_df['visit_date'].dt.month
patients_df['quarter'] = patients_df['visit_date'].dt.quarter
patients_df['day_of_week'] = patients_df['visit_date'].dt.dayofweek

disease_df.dtypes

disease_df['month']=pd.to_datetime(disease_df['month'])

disease_df.head(3)

disease_df['year'] = disease_df['month'].dt.year
disease_df['month_num'] = disease_df['month'].dt.month

"""Perfoming Exploitory Data Analysis(EDA)"""

#### patients visiting dsays analysys
patient_facility_counts = patients_df.groupby('facility_id').size().reset_index(name='total_visits')
print(f"Total unique facilities with patient visits: {patient_facility_counts.shape[0]}")
print(f"Average visits per facility: {patient_facility_counts['total_visits'].mean():.0f}")

top_diagnoses = patients_df['diagnosis'].value_counts().head(10)
print("\nTop 10 Diagnoses:")
print(top_diagnoses)

treatment_counts = patients_df.groupby(['diagnosis', 'treatment']).size().reset_index(name='count')
print(f"\nUnique diagnosis-treatment combinations: {treatment_counts.shape[0]}")
# print(f"tratement_counts: {treatment_counts}")

"""For inventory analysis"""

inventory_df.dtypes

inventory_df['last_restock_date']=pd.to_datetime(inventory_df['last_restock_date'])

inventory_df['days_since_restock'] = (datetime.now() - inventory_df['last_restock_date']).dt.days
inventory_df['stock_status'] = inventory_df['stock_level'] - inventory_df['reorder_level']
inventory_df['at_risk'] = inventory_df['stock_status'] <= 0

inventory_df.head(2)

inventory_df['at_risk'].value_counts(normalize=True)

print(f"Total inventory items: {inventory_df.shape[0]:,}")
print(f"Items at or below reorder level: {inventory_df['at_risk'].sum():,} ({inventory_df['at_risk'].mean()*100:.1f}%)")
print(f"Average days since last restock: {inventory_df['days_since_restock'].mean():.0f}")

"""Disease Pattern Analysis"""

disease_df.head(3)

disease_summary = disease_df.groupby('disease').agg({
    'cases_reported': 'sum',
    'deaths': 'sum'
}).sort_values('cases_reported', ascending=False)

disease_summary['mortality_rate'] = (disease_summary['deaths'] / disease_summary['cases_reported'] * 100).round(2)
print("\nTop 10 Diseases by Cases:")
print(disease_summary.head(10))

"""Facility analysis"""

nigeria_phc_df=nigeria_df.copy()
print(f"Total facilities: {nigeria_phc_df.shape[0]:,}")
print(f"Operational facilities: {(nigeria_phc_df['operational_status'] == 'Functional').sum():,}")
print(f"Non-functional facilities: {(nigeria_phc_df['operational_status'] == 'Non-Functional').sum():,}")

facility_summary = nigeria_phc_df.groupby('state').agg({
    'facility_id': 'count',
    'average_daily_patients': 'mean',
    'health_workers': 'mean'
}).round(2)
print("\nFacility distribution by state (top 10):")
print(facility_summary.sort_values('facility_id', ascending=False).head(10))

"""FEATURE ENGINEERING"""



inventory_df.head(3)

inventory_df['item_name'].unique()

inventory_df['item_name'].nunique()

cutoff_date = patients_df['visit_date'].max() - pd.Timedelta(days=30)
historical_patients = patients_df[patients_df['visit_date'] <= cutoff_date]

#Daily patient statistics (historical)
facility_patient_stats = historical_patients.groupby('facility_id').agg({
    'patient_id': 'count',
    'visit_date': lambda x: (x.max() - x.min()).days
}).reset_index()
facility_patient_stats.columns = ['facility_id', 'total_historical_visits', 'days_of_data']
facility_patient_stats['avg_daily_patients_historical'] = (
    facility_patient_stats['total_historical_visits'] /
    (facility_patient_stats['days_of_data'] + 1)
)

# Treatment patterns (historical)
treatment_patterns = historical_patients.groupby(['facility_id', 'treatment']).size().reset_index(name='count')
treatment_diversity = treatment_patterns.groupby('facility_id').agg({
    'treatment': 'count',
    'count': ['sum', 'mean', 'std']
}).reset_index()
treatment_diversity.columns = ['facility_id', 'unique_treatments', 'total_treatments',
                                'avg_treatment_freq', 'std_treatment_freq']

# Diagnosis patterns
diagnosis_patterns = historical_patients.groupby(['facility_id', 'diagnosis']).size().reset_index(name='count')
diagnosis_diversity = diagnosis_patterns.groupby('facility_id')['diagnosis'].count().reset_index()
diagnosis_diversity.columns = ['facility_id', 'unique_diagnoses']

# 3.2: Disease burden (HISTORICAL - exclude recent months)
disease_cutoff = disease_df['month'].max() - pd.DateOffset(months=1)
historical_diseases = disease_df[disease_df['month'] <= disease_cutoff]

disease_burden = historical_diseases.groupby('facility_id').agg({
    'cases_reported': ['sum', 'mean', 'std', 'max'],
    'deaths': ['sum', 'mean'],
    'disease': 'count'
}).reset_index()
disease_burden.columns = ['facility_id', 'total_cases_hist', 'avg_monthly_cases',
                          'std_monthly_cases', 'max_monthly_cases',
                          'total_deaths_hist', 'avg_monthly_deaths', 'months_reported']

# Seasonal disease patterns
historical_diseases['season'] = historical_diseases['month_num'].map({
    12: 'dry', 1: 'dry', 2: 'dry', 3: 'dry',
    4: 'wet', 5: 'wet', 6: 'wet', 7: 'wet', 8: 'wet', 9: 'wet',
    10: 'dry', 11: 'dry'
})
seasonal_cases = historical_diseases.groupby(['facility_id', 'season'])['cases_reported'].mean().reset_index()
seasonal_pivot = seasonal_cases.pivot(index='facility_id', columns='season', values='cases_reported').reset_index()
seasonal_pivot.columns = ['facility_id', 'avg_cases_dry_season', 'avg_cases_wet_season']
seasonal_pivot.fillna(0, inplace=True)

# 3.3: Health worker capacity
worker_capacity = health_workers_df.groupby('facility_id').agg({
    'worker_id': 'count',
    'years_experience': ['mean', 'max', 'min'],
    'availability_status': lambda x: (x == 'Present').sum(),
    'role': lambda x: x.value_counts().to_dict()
}).reset_index()
worker_capacity.columns = ['facility_id', 'total_workers', 'avg_experience',
                           'max_experience', 'min_experience', 'present_workers', 'role_dist']

# Count doctors and nurses
def count_doctors(role_dict):
    return role_dict.get('Doctor', 0) if isinstance(role_dict, dict) else 0

def count_nurses(role_dict):
    return role_dict.get('Nurse', 0) if isinstance(role_dict, dict) else 0

worker_capacity['num_doctors'] = worker_capacity['role_dist'].apply(count_doctors)
worker_capacity['num_nurses'] = worker_capacity['role_dist'].apply(count_nurses)
worker_capacity['staff_presence_rate'] = worker_capacity['present_workers'] / (worker_capacity['total_workers'] + 1)
worker_capacity = worker_capacity.drop('role_dist', axis=1)

# 3.4: Facility infrastructure
facility_features = nigeria_phc_df[['facility_id', 'operational_status', 'number_of_beds',
                                     'average_daily_patients', 'health_workers',
                                     'state', 'lga', 'ownership']].copy()

# Encode categorical features
le_operational = LabelEncoder()
le_ownership = LabelEncoder()
facility_features['operational_encoded'] = le_operational.fit_transform(facility_features['operational_status'])
facility_features['ownership_encoded'] = le_ownership.fit_transform(facility_features['ownership'])

# Calculate capacity metrics
facility_features['patients_per_bed'] = (
    facility_features['average_daily_patients'] / (facility_features['number_of_beds'] + 1)
)
facility_features['patients_per_worker'] = (
    facility_features['average_daily_patients'] / (facility_features['health_workers'] + 1)
)

# 3.5: Inventory-specific features (NO DIRECT STOCK LEVEL)
inventory_features = inventory_df.copy()
inventory_features['days_since_restock'] = (datetime.now() - inventory_features['last_restock_date']).dt.days
inventory_features['restock_frequency'] = np.where(
    inventory_features['days_since_restock'] > 0,
    365 / inventory_features['days_since_restock'],
    0
)

inventory_df['item_name'].unique()

def categorize_item(item_name):
    """
    Categorizes an inventory item into a high-level consumption or treatment group.
    This helps link item usage to specific patient visit types (diagnoses).
    """
    item_lower = str(item_name).lower()

    # --- Preventative & Vaccination ---
    if any(word in item_lower for word in ['vaccine', 'vaccination', 'syringes']):
        return 'vaccine_related'

    # --- Infections & Malaria ---
    elif any(word in item_lower for word in ['antibiotic', 'antifungal', 'topical']):
        return 'antibiotics_wound_care'
    elif any(word in item_lower for word in ['malaria', 'artemether', 'act']):
        return 'antimalarial'

    # --- Chronic Diseases (NCDs) ---
    elif any(word in item_lower for word in ['insulin', 'metformin', 'amlodipine']):
        return 'ncd_medication'

    # --- Rehydration & Supplements ---
    elif any(word in item_lower for word in ['ors', 'rehydration', 'zinc', 'nutrition']):
        return 'rehydration_supplement'
    elif any(word in item_lower for word in ['iv', 'fluid']):
        return 'iv_fluid'

    # --- Maternal & Delivery Care ---
    elif any(word in item_lower for word in ['delivery', 'anc', 'sanitary']):
        return 'maternal_delivery'

    # --- General Consumables & PPE (Consumed per visit) ---
    elif any(word in item_lower for word in ['glove', 'mask', 'ppe', 'sanitizer', 'wool']):
        return 'ppe_consumables'

    # --- Diagnostics & Monitoring ---
    elif any(word in item_lower for word in ['test kit', 'monitor', 'thermometer', 'stethoscope']):
        return 'diagnostics_triage'

    # --- Symptomatic Relief & Other Drugs ---
    elif any(word in item_lower for word in ['paracetamol', 'cough syrup', 'pain relief', 'deworming']):
        return 'symptomatic_drugs'

    # --- Administrative & Infrastructure ---
    elif any(word in item_lower for word in ['generator', 'record book', 'leaflet', 'referral', 'kit', 'disposal']):
        return 'admin_infrastructure'

    else:
        return 'other'

inventory_features['item_category'] = inventory_features['item_name'].apply(categorize_item)
le_category = LabelEncoder()
inventory_features['item_category_encoded'] = le_category.fit_transform(inventory_features['item_category'])

print("\n[STEP 4] Merging Features...")

# Start with inventory
enhanced_df = inventory_features[['item_id', 'facility_id', 'item_name', 'stock_level',
                                   'reorder_level', 'days_since_restock',
                                   'restock_frequency', 'item_category_encoded']].copy()

# Merge facility features
enhanced_df = enhanced_df.merge(facility_features, on='facility_id', how='left')

# Merge patient patterns
enhanced_df = enhanced_df.merge(facility_patient_stats, on='facility_id', how='left')
enhanced_df = enhanced_df.merge(treatment_diversity, on='facility_id', how='left')
enhanced_df = enhanced_df.merge(diagnosis_diversity, on='facility_id', how='left')

# Merge disease burden
enhanced_df = enhanced_df.merge(disease_burden, on='facility_id', how='left')
enhanced_df = enhanced_df.merge(seasonal_pivot, on='facility_id', how='left')

# Merge worker capacity
enhanced_df = enhanced_df.merge(worker_capacity, on='facility_id', how='left')

# Fill missing values with 0
enhanced_df.fillna(0, inplace=True)

print(f"✓ Enhanced dataset shape: {enhanced_df.shape}")
print(f"✓ Features created: {enhanced_df.shape[1]}")

print("\n[STEP 5] Creating Target Variables...")

# Calculate realistic consumption rate
# Base it on: facility patient load + disease burden + item category
enhanced_df['base_consumption_rate'] = (
    enhanced_df['avg_daily_patients_historical'] * 0.3 +  # Historical patient pattern
    enhanced_df['avg_monthly_cases'] / 30 * 0.4 +  # Disease burden
    enhanced_df['reorder_level'] / 60 * 0.3  # Expected turnover based on reorder point
)

# Add randomness and seasonality
np.random.seed(42)
enhanced_df['consumption_variation'] = np.random.uniform(0.8, 1.2, len(enhanced_df))
enhanced_df['estimated_daily_consumption'] = (
    enhanced_df['base_consumption_rate'] * enhanced_df['consumption_variation']
).clip(lower=0.1)

# The calculation result is a NumPy array
days_calc = np.where(
    enhanced_df['estimated_daily_consumption'] > 0,
    (enhanced_df['stock_level'] - enhanced_df['reorder_level']) / enhanced_df['estimated_daily_consumption'],
    999
)

# Use the NumPy clip function with the resulting array
enhanced_df['days_until_stockout'] = np.clip(
    days_calc,
    a_min=0,
    a_max=365
)

# Create balanced target: Stock-out in next 30 days
# Use more conservative threshold to get better balance
enhanced_df['stockout_30days'] = (
    (enhanced_df['days_until_stockout'] <= 30) |
    (enhanced_df['stock_level'] <= enhanced_df['reorder_level'] * 1.2)
).astype(int)

print("\n✓ Target Variable Distribution:")
print(enhanced_df['stockout_30days'].value_counts())
print(f"Stock-out rate: {enhanced_df['stockout_30days'].mean()*100:.1f}%")

# If still imbalanced, adjust threshold
if enhanced_df['stockout_30days'].mean() > 0.7:
    print("\n⚠ Still imbalanced, adjusting threshold...")
    threshold = enhanced_df['days_until_stockout'].quantile(0.5)
    enhanced_df['stockout_30days'] = (enhanced_df['days_until_stockout'] <= threshold).astype(int)
    print(f"New threshold: {threshold:.1f} days")
    print(enhanced_df['stockout_30days'].value_counts())

print("\n[STEP 6] Preparing Features...")

# Select features that don't leak information about current stock status
feature_columns = [
    # Item characteristics
    'reorder_level', 'days_since_restock', 'restock_frequency', 'item_category_encoded',

    # Facility infrastructure
    'operational_encoded', 'ownership_encoded', 'number_of_beds',
    'average_daily_patients', 'health_workers', 'patients_per_bed', 'patients_per_worker',

    # Historical patient patterns
    'avg_daily_patients_historical', 'unique_treatments', 'total_treatments',
    'avg_treatment_freq', 'unique_diagnoses',

    # Historical disease burden
    'total_cases_hist', 'avg_monthly_cases', 'std_monthly_cases', 'max_monthly_cases',
    'total_deaths_hist', 'avg_monthly_deaths', 'months_reported',
    'avg_cases_dry_season', 'avg_cases_wet_season',

    # Worker capacity
    'total_workers', 'avg_experience', 'max_experience', 'min_experience',
    'present_workers', 'num_doctors', 'num_nurses', 'staff_presence_rate'
]

available_features = [f for f in feature_columns if f in enhanced_df.columns]
print(f"✓ Using {len(available_features)} features (removed leakage)")

X = enhanced_df[available_features].copy()
y_class = enhanced_df['stockout_30days'].copy()
y_reg = enhanced_df['days_until_stockout'].copy()

print("\n[STEP 7] Handling Class Imbalance...")

# Split data first
X_train, X_test, y_train, y_test = train_test_split(
    X, y_class, test_size=0.2, random_state=42, stratify=y_class
)

print(f"\nOriginal training distribution:")
print(y_train.value_counts())

#Apply SMOTE for oversampling minority + Random Undersampling for majority
# This creates a balanced dataset
smote = SMOTE(random_state=42)  # Oversample minority to 50% of majority
undersample = RandomUnderSampler( random_state=42)  # Then balance further

# Apply both
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
X_train_balanced, y_train_balanced = undersample.fit_resample(X_train_resampled, y_train_resampled)

print(f"\nBalanced training distribution:")
print(pd.Series(y_train_balanced).value_counts())
print(f"Balance ratio: {pd.Series(y_train_balanced).value_counts().min() / pd.Series(y_train_balanced).value_counts().max():.2f}")

print("\n[STEP 8] Training Models with Cross-Validation...")

# 8.1: Random Forest with Class Weights
print("\n--- Random Forest Classifier (with CV) ---")
rf_classifier = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=20,
    min_samples_leaf=10,
    class_weight='balanced',  # Handle any remaining imbalance
    random_state=42,
    n_jobs=-1
)

#Cross-validation on balanced data
cv_scores = cross_val_score(rf_classifier, X_train_balanced, y_train_balanced,
                            cv=5, scoring='roc_auc', n_jobs=-1)
print(f"Cross-Validation ROC-AUC Scores: {cv_scores}")
print(f"Mean CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

# Train on full balanced data
rf_classifier.fit(X_train_balanced, y_train_balanced)

# Predict on original test set
y_pred_rf = rf_classifier.predict(X_test)
y_pred_proba_rf = rf_classifier.predict_proba(X_test)[:, 1]

print("\nRandom Forest Performance on Test Set:")
print(classification_report(y_test, y_pred_rf, target_names=['No Stock-out', 'Stock-out']))
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_rf):.4f}")

# Feature importance
feature_importance_rf = pd.DataFrame({
    'feature': available_features,
    'importance': rf_classifier.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 15 Most Important Features:")
print(feature_importance_rf.head(15))

# 8.2: XGBoost with Scale Pos Weight
print("\n--- XGBoost Classifier (with CV) ---")

# Calculate scale_pos_weight for imbalance
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
print(f"Scale pos weight: {scale_pos_weight:.2f}")

xgb_classifier = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=8,
    learning_rate=0.05,
    scale_pos_weight=scale_pos_weight,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='logloss'
)

# Cross-validation
cv_scores_xgb = cross_val_score(xgb_classifier, X_train_balanced, y_train_balanced,
                                 cv=5, scoring='roc_auc', n_jobs=-1)
print(f"Cross-Validation ROC-AUC Scores: {cv_scores_xgb}")
print(f"Mean CV ROC-AUC: {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std():.4f})")

# Train
xgb_classifier.fit(X_train_balanced, y_train_balanced)

# Predict
y_pred_xgb = xgb_classifier.predict(X_test)
y_pred_proba_xgb = xgb_classifier.predict_proba(X_test)[:, 1]

print("\nXGBoost Performance on Test Set:")
print(classification_report(y_test, y_pred_xgb, target_names=['No Stock-out', 'Stock-out']))
print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}")

print("\n[STEP 9] Training Regression Model...")

# Use same train-test split but for regression target
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X, y_reg, test_size=0.2, random_state=42
)

#Random Forest Regressor with CV
print("\n--- Random Forest Regressor (with CV) ---")
rf_regressor = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1
)

#Cross-validation (negative MAE)
cv_scores_reg = cross_val_score(rf_regressor, X_train_reg, y_train_reg,
                                 cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
print(f"Cross-Validation MAE: {-cv_scores_reg.mean():.2f} days (+/- {cv_scores_reg.std():.2f})")

# Train
rf_regressor.fit(X_train_reg, y_train_reg)

# Predict
y_pred_reg = rf_regressor.predict(X_test_reg)

mae_rf = mean_absolute_error(y_test_reg, y_pred_reg)
r2_rf = r2_score(y_test_reg, y_pred_reg)

print(f"\nRandom Forest Regressor Performance:")
print(f"Mean Absolute Error: {mae_rf:.2f} days")
print(f"R² Score: {r2_rf:.4f}")

# XGBoost Regressor
print("\n--- XGBoost Regressor (with CV) ---")
xgb_regressor = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=8,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Cross-validation
cv_scores_xgb_reg = cross_val_score(xgb_regressor, X_train_reg, y_train_reg,
                                     cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)
print(f"Cross-Validation MAE: {-cv_scores_xgb_reg.mean():.2f} days (+/- {cv_scores_xgb_reg.std():.2f})")

# Train
xgb_regressor.fit(X_train_reg, y_train_reg)

# Predict
y_pred_xgb_reg = xgb_regressor.predict(X_test_reg)

mae_xgb = mean_absolute_error(y_test_reg, y_pred_xgb_reg)
r2_xgb = r2_score(y_test_reg, y_pred_xgb_reg)

print(f"\nXGBoost Regressor Performance:")
print(f"Mean Absolute Error: {mae_xgb:.2f} days")
print(f"R² Score: {r2_xgb:.4f}")

inventory_df.head(3)

inventory_df['facility_id'].nunique()

import pickle
import joblib

with open('xgb_classifier.pkl', 'wb') as f:
    pickle.dump(xgb_classifier, f)

with open('xgb_regressor.pkl', 'wb') as f:
    pickle.dump(xgb_regressor, f)

classsifier=joblib.load('/content/xgb_classifier.pkl')
regressor=joblib.load('/content/xgb_regressor.pkl')

{
  "facility_id": "PHC_00003",
  "item_name": "Antimalarials",
  "current_stock_level": 45,
  "reorder_level": 100,
  "last_restock_date": "2024-08-15"
}

"""Generating dummy files"""

# # Convert dates
# patients_df['visit_date'] = pd.to_datetime(patients_df['visit_date'])
# disease_df['month'] = pd.to_datetime(disease_df['month'])

# # Calculate features per facility
# cutoff_date = patients_df['visit_date'].max() - pd.Timedelta(days=30)
# historical_patients = patients_df[patients_df['visit_date'] <= cutoff_date]

# # Patient features
# patient_features = historical_patients.groupby('facility_id').agg({
#     'patient_id': 'count',
#     'visit_date': lambda x: (x.max() - x.min()).days
# }).reset_index()
# patient_features.columns = ['facility_id', 'total_historical_visits', 'days_of_data']
# patient_features['avg_daily_patients_historical'] = patient_features['total_historical_visits'] / (patient_features['days_of_data'] + 1)

# # Treatment diversity
# treatment_patterns = historical_patients.groupby(['facility_id', 'treatment']).size().reset_index(name='count')
# treatment_diversity = treatment_patterns.groupby('facility_id').agg({
#     'treatment': 'count',
#     'count': ['sum', 'mean']
# }).reset_index()
# treatment_diversity.columns = ['facility_id', 'unique_treatments', 'total_treatments', 'avg_treatment_freq']

# # Diagnosis diversity
# diagnosis_diversity = historical_patients.groupby('facility_id')['diagnosis'].nunique().reset_index()
# diagnosis_diversity.columns = ['facility_id', 'unique_diagnoses']

# # Disease burden
# disease_burden = disease_df.groupby('facility_id').agg({
#     'cases_reported': ['sum', 'mean', 'std', 'max'],
#     'deaths': ['sum', 'mean'],
#     'disease': 'count'
# }).reset_index()
# disease_burden.columns = ['facility_id', 'total_cases_hist', 'avg_monthly_cases',
#                           'std_monthly_cases', 'max_monthly_cases',
#                           'total_deaths_hist', 'avg_monthly_deaths', 'months_reported']

# # Seasonal patterns
# disease_df['season'] = disease_df['month'].dt.month.map({
#     12: 'dry', 1: 'dry', 2: 'dry', 3: 'dry',
#     4: 'wet', 5: 'wet', 6: 'wet', 7: 'wet', 8: 'wet', 9: 'wet',
#     10: 'dry', 11: 'dry'
# })
# seasonal = disease_df.groupby(['facility_id', 'season'])['cases_reported'].mean().reset_index()
# seasonal_pivot = seasonal.pivot(index='facility_id', columns='season', values='cases_reported').reset_index()
# seasonal_pivot.columns = ['facility_id', 'avg_cases_dry_season', 'avg_cases_wet_season']

# # Worker capacity
# worker_capacity = health_workers_df.groupby('facility_id').agg({
#     'worker_id': 'count',
#     'years_experience': ['mean', 'max', 'min'],
#     'availability_status': lambda x: (x == 'Present').sum()
# }).reset_index()
# worker_capacity.columns = ['facility_id', 'total_workers', 'avg_experience',
#                            'max_experience', 'min_experience', 'present_workers']

# # Count doctors and nurses
# role_counts = health_workers_df.groupby(['facility_id', 'role']).size().unstack(fill_value=0).reset_index()
# if 'Doctor' in role_counts.columns:
#     worker_capacity['num_doctors'] = role_counts['Doctor']
# if 'Nurse' in role_counts.columns:
#     worker_capacity['num_nurses'] = role_counts['Nurse']

# worker_capacity['staff_presence_rate'] = worker_capacity['present_workers'] / worker_capacity['total_workers']

# # Facility info
# from sklearn.preprocessing import LabelEncoder
# le_operational = LabelEncoder()
# le_ownership = LabelEncoder()

# nigeria_phc_df['operational_encoded'] = le_operational.fit_transform(nigeria_phc_df['operational_status'])
# nigeria_phc_df['ownership_encoded'] = le_ownership.fit_transform(nigeria_phc_df['ownership'])
# nigeria_phc_df['patients_per_bed'] = nigeria_phc_df['average_daily_patients'] / (nigeria_phc_df['number_of_beds'] + 1)
# nigeria_phc_df['patients_per_worker'] = nigeria_phc_df['average_daily_patients'] / (nigeria_phc_df['health_workers'] + 1)

# facility_info = nigeria_phc_df[['facility_id', 'operational_encoded', 'ownership_encoded',
#                                  'number_of_beds', 'average_daily_patients', 'health_workers',
#                                  'patients_per_bed', 'patients_per_worker']]

# # Merge all features
# facility_features = facility_info.copy()
# facility_features = facility_features.merge(patient_features, on='facility_id', how='left')
# facility_features = facility_features.merge(treatment_diversity, on='facility_id', how='left')
# facility_features = facility_features.merge(diagnosis_diversity, on='facility_id', how='left')
# facility_features = facility_features.merge(disease_burden, on='facility_id', how='left')
# facility_features = facility_features.merge(seasonal_pivot, on='facility_id', how='left')
# facility_features = facility_features.merge(worker_capacity, on='facility_id', how='left')

# # Fill missing values
# facility_features.fillna(0, inplace=True)

# # Save
# facility_features.to_csv('facility_features_precalculated.csv', index=False)
# print(f"✅ Created facility features for {len(facility_features)} facilities")







daily_visits = patients_df.groupby(['facility_id', 'visit_date']).size().reset_index(name='daily_visits')
facility_visit_stats = daily_visits.groupby('facility_id')['daily_visits'].agg(['mean', 'std', 'max']).reset_index()
facility_visit_stats.columns = ['facility_id', 'avg_daily_visits', 'std_daily_visits', 'max_daily_visits']

# Treatment frequency per facility
treatment_freq = patients_df.groupby(['facility_id', 'treatment']).size().reset_index(name='treatment_count')
treatment_pivot = treatment_freq.pivot_table(index='facility_id', columns='treatment', values='treatment_count', fill_value=0)
treatment_pivot.columns = [f'treatment_freq_{col}' for col in treatment_pivot.columns]
treatment_pivot = treatment_pivot.reset_index()

# 4.2: Disease burden features
disease_burden = disease_df.groupby('facility_id').agg({
    'cases_reported': ['sum', 'mean', 'max'],
    'deaths': ['sum', 'mean']
}).reset_index()
disease_burden.columns = ['facility_id', 'total_cases', 'avg_monthly_cases', 'max_monthly_cases',
                          'total_deaths', 'avg_monthly_deaths']

# Recent disease trends (last 3 months)
recent_date = disease_df['month'].max()
three_months_ago = recent_date - pd.DateOffset(months=3)
recent_diseases = disease_df[disease_df['month'] >= three_months_ago]
recent_disease_burden = recent_diseases.groupby('facility_id')['cases_reported'].sum().reset_index()
recent_disease_burden.columns = ['facility_id', 'recent_3month_cases']

# 4.3: Health worker capacity
health_workers_df=workers_df.copy()
worker_capacity = health_workers_df.groupby('facility_id').agg({
    'worker_id': 'count',
    'years_experience': 'mean',
    'availability_status': lambda x: (x == 'Present').sum()
}).reset_index()
worker_capacity.columns = ['facility_id', 'total_workers', 'avg_experience', 'present_workers']

print("Merging features with inventory...")
inventory_enhanced = inventory_df.copy()

# Merge with facility information
inventory_enhanced = inventory_enhanced.merge(nigeria_phc_df[['facility_id', 'operational_status',
                                                              'number_of_beds', 'average_daily_patients',
                                                              'health_workers', 'state', 'lga']],
                                              on='facility_id', how='left')

# Merge with patient visit statistics
inventory_enhanced = inventory_enhanced.merge(facility_visit_stats, on='facility_id', how='left')

# Merge with disease burden
inventory_enhanced = inventory_enhanced.merge(disease_burden, on='facility_id', how='left')
inventory_enhanced = inventory_enhanced.merge(recent_disease_burden, on='facility_id', how='left')

# Merge with worker capacity
inventory_enhanced = inventory_enhanced.merge(worker_capacity, on='facility_id', how='left')

# Fill missing values
inventory_enhanced.fillna(0, inplace=True)

print(f"✓ Enhanced inventory dataset: {inventory_enhanced.shape}")

print("Creating target variables...")

# Target 1: Stock-out risk classification (High/Medium/Low)
inventory_enhanced['stock_ratio'] = inventory_enhanced['stock_level'] / (inventory_enhanced['reorder_level'] + 1)
inventory_enhanced['stockout_risk'] = pd.cut(inventory_enhanced['stock_ratio'],
                                             bins=[-np.inf, 0.5, 1.0, np.inf],
                                             labels=['High', 'Medium', 'Low'])

#Target 2: Days until stock-out (regression)
# Estimate consumption rate based on patient visits and item type
inventory_enhanced['estimated_daily_consumption'] = (
    inventory_enhanced['avg_daily_visits'] * 0.3 +  # Base consumption
    inventory_enhanced['recent_3month_cases'] / 90 * 0.5  # Disease-driven consumption
)
inventory_enhanced['estimated_daily_consumption'] = inventory_enhanced['estimated_daily_consumption'].replace(0, 1)

inventory_enhanced['days_until_stockout'] = (
    inventory_enhanced['stock_level'] / inventory_enhanced['estimated_daily_consumption']
).clip(lower=0, upper=365)

# Binary classification: Will stock out in next 30 days?
inventory_enhanced['stockout_30days'] = (inventory_enhanced['days_until_stockout'] <= 30).astype(int)

print(f"✓ Stock-out risk distribution:")
print(inventory_enhanced['stockout_risk'].value_counts())
print(f"\n✓ Items at risk of stock-out in 30 days: {inventory_enhanced['stockout_30days'].sum()} ({inventory_enhanced['stockout_30days'].mean()*100:.1f}%)")

print("\n[STEP 5] Building Stock-Out Risk Classification Model...")

# Prepare features for classification
classification_features = [
    'stock_level', 'reorder_level', 'days_since_restock', 'stock_status',
    'number_of_beds', 'average_daily_patients', 'health_workers',
    'avg_daily_visits', 'std_daily_visits', 'max_daily_visits',
    'total_cases', 'avg_monthly_cases', 'max_monthly_cases',
    'total_deaths', 'avg_monthly_deaths', 'recent_3month_cases',
    'total_workers', 'avg_experience', 'present_workers'
]

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, r2_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

le_operational = LabelEncoder()
inventory_enhanced['operational_status_encoded'] = le_operational.fit_transform(inventory_enhanced['operational_status'])
classification_features.append('operational_status_encoded')

X_class = inventory_enhanced[classification_features].copy()
y_class = inventory_enhanced['stockout_30days'].copy()

X_class.fillna(0, inplace=True)

X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(
    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class
)

print(f"Training set: {X_train_class.shape[0]} samples")
print(f"Test set: {X_test_class.shape[0]} samples")
print(f"Class distribution in training: {y_train_class.value_counts().to_dict()}")

print("\n--- Training Random Forest Classifier ---")
rf_classifier = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)

rf_classifier.fit(X_train_class, y_train_class)
y_pred_rf = rf_classifier.predict(X_test_class)

print("\nRandom Forest Performance:")
print(classification_report(y_test_class, y_pred_rf, target_names=['No Stock-out', 'Stock-out']))

# Feature importance
feature_importance_rf = pd.DataFrame({
    'feature': classification_features,
    'importance': rf_classifier.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 10 Most Important Features:")
print(feature_importance_rf.head(10))

print("\n--- Training XGBoost Classifier ---")
xgb_classifier = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42,
    eval_metric='logloss'
)

xgb_classifier.fit(X_train_class, y_train_class)
y_pred_xgb = xgb_classifier.predict(X_test_class)

print("\nXGBoost Performance:")
print(classification_report(y_test_class, y_pred_xgb, target_names=['No Stock-out', 'Stock-out']))

print("\n[STEP 6] Building Days Until Stock-Out Regression Model...")

# Prepare data for regression
X_reg = inventory_enhanced[classification_features].copy()
y_reg = inventory_enhanced['days_until_stockout'].copy()

# Split data
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

print("\n--- Training Gradient Boosting Regressor ---")
gb_regressor = GradientBoostingRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42
)

gb_regressor.fit(X_train_reg, y_train_reg)
y_pred_gb = gb_regressor.predict(X_test_reg)

mae_gb = mean_absolute_error(y_test_reg, y_pred_gb)
r2_gb = r2_score(y_test_reg, y_pred_gb)

print(f"\nGradient Boosting Performance:")
print(f"Mean Absolute Error: {mae_gb:.2f} days")
print(f"R² Score: {r2_gb:.4f}")

# 6.2: XGBoost Regressor
print("\n--- Training XGBoost Regressor ---")
xgb_regressor = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)

xgb_regressor.fit(X_train_reg, y_train_reg)
y_pred_xgb_reg = xgb_regressor.predict(X_test_reg)

mae_xgb = mean_absolute_error(y_test_reg, y_pred_xgb_reg)
r2_xgb = r2_score(y_test_reg, y_pred_xgb_reg)

print(f"\nXGBoost Regressor Performance:")
print(f"Mean Absolute Error: {mae_xgb:.2f} days")
print(f"R² Score: {r2_xgb:.4f}")

print("\n[STEP 7] Generating Actionable Insights...")

# Create predictions dataframe
predictions_df = inventory_enhanced[['item_id', 'facility_id', 'item_name', 'stock_level',
                                     'reorder_level', 'days_since_restock']].copy()

# Add predictions
predictions_df['stockout_probability'] = xgb_classifier.predict_proba(X_class)[:, 1]
predictions_df['predicted_days_until_stockout'] = xgb_regressor.predict(X_reg)
predictions_df['actual_days_until_stockout'] = inventory_enhanced['days_until_stockout']

# Priority scoring
predictions_df['priority_score'] = (
    predictions_df['stockout_probability'] * 0.5 +
    (1 / (predictions_df['predicted_days_until_stockout'] + 1)) * 0.5
)

# Sort by priority
high_priority = predictions_df.sort_values('priority_score', ascending=False).head(20)

print("\n=== TOP 20 HIGH-PRIORITY STOCK-OUT RISKS ===")
print(high_priority[['facility_id', 'item_name', 'stock_level',
                     'stockout_probability', 'predicted_days_until_stockout',
                     'priority_score']].to_string())

facility_risk = predictions_df.groupby('facility_id').agg({
    'stockout_probability': 'mean',
    'predicted_days_until_stockout': 'min',
    'priority_score': 'max',
    'item_id': 'count'
}).reset_index()
facility_risk.columns = ['facility_id', 'avg_stockout_prob', 'min_days_to_stockout',
                         'max_priority_score', 'total_items']

facility_risk = facility_risk.sort_values('max_priority_score', ascending=False).head(10)

print("\n=== TOP 10 FACILITIES AT HIGHEST RISK ===")
print(facility_risk.to_string())

print("\n[STEP 8] Creating Visualizations...")

fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Plot 1: Stock-out risk distribution
risk_counts = inventory_enhanced['stockout_risk'].value_counts()
axes[0, 0].bar(risk_counts.index, risk_counts.values, color=['red', 'orange', 'green'])
axes[0, 0].set_title('Stock-Out Risk Distribution', fontsize=12, fontweight='bold')
axes[0, 0].set_ylabel('Number of Items')

# Plot 2: Feature importance
top_features = feature_importance_rf.head(10)
axes[0, 1].barh(top_features['feature'], top_features['importance'])
axes[0, 1].set_title('Top 10 Feature Importance (Random Forest)', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('Importance')

sample_indices = np.random.choice(len(y_test_reg), min(500, len(y_test_reg)), replace=False)
axes[1, 0].scatter(y_test_reg.iloc[sample_indices], y_pred_xgb_reg[sample_indices], alpha=0.5)
axes[1, 0].plot([0, 365], [0, 365], 'r--', label='Perfect Prediction')
axes[1, 0].set_title('Predicted vs Actual Days Until Stock-Out', fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('Actual Days')
axes[1, 0].set_ylabel('Predicted Days')
axes[1, 0].legend()

# Plot 4: Priority Score Distribution
axes[1, 1].hist(predictions_df['priority_score'], bins=50, edgecolor='black')
axes[1, 1].set_title('Priority Score Distribution', fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('Priority Score')
axes[1, 1].set_ylabel('Frequency')

plt.tight_layout()
plt.savefig('stockout_prediction_analysis.png', dpi=300, bbox_inches='tight')
print("✓ Visualization saved as 'stockout_prediction_analysis.png'")



